{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982d0e58-0a27-420b-a2ee-e211de9770e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Credenciales\n",
    "sec = dbutils.secrets.get(\"scpcumpsccrit002\",\"gscp1glbsp4cumpscauth100\").split(\"[###]\")\n",
    "client_id = sec[1]\n",
    "client_secret = sec[0]\n",
    "\n",
    "client_endpoint = \"https://login.microsoftonline.com/35595a02-4d6d-44ac-99e1-f9ab4cd872db/oauth2/token\"\n",
    " \n",
    "storage_account = \"gscp1weustacumpsccrit100\"\n",
    " \n",
    "# Configurar Spark para autenticación OAuth de Azure Data Lake Storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", client_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "096e69fe-c267-4134-b705-7f4d87461a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/x949854@santanderglobaltech.com/cumplimiento/swift/SWIFTUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83ae5709-174f-465a-af43-8e631c8c580d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import shutil  # local fallback copy\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate().newSession()\n",
    "logger = SWIFTParserLogger.getSWIFTParserLogger('INFO')\n",
    "\n",
    "\n",
    "def main(adls_input_base_dir: str, adls_output_base_dir: str, date: str, local_dir: str):\n",
    "    dbutils = get_dbutils(spark)\n",
    "\n",
    "    #NEW\n",
    "\n",
    "    def copy_to_staging(local_path: str, staging_dir: str):\n",
    "            \"\"\"\n",
    "            Copy local_path (absolute filesystem path) to staging_dir.\n",
    "            - Tries dbutils.fs.cp using file: prefix.\n",
    "            - If that fails, tries to map staging_dir to local fs (/dbfs/...) and uses shutil.copy2.\n",
    "            - Non-fatal: logs errors but does not raise.\n",
    "            \"\"\"\n",
    "            if not staging_dir:\n",
    "                logger.debug(\"No staging_dir provided; skipping staging copy.\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                local_path = str(local_path)\n",
    "                base_name = os.path.basename(local_path)\n",
    "                # Ensure target path in staging\n",
    "                staging_dir = staging_dir.rstrip('/')\n",
    "                target_staging_path = f\"{staging_dir}/{base_name}\"\n",
    "\n",
    "                # Build a file: URI for dbutils if needed\n",
    "                local_volume_path = local_path\n",
    "                if not local_dir.startswith('file:') and not local_dir.startswith('/dbfs'):\n",
    "                    local_volume_path = f\"file:{local_path}\"\n",
    "\n",
    "                # Try dbutils.fs.cp first (works in Databricks for many schemes)\n",
    "                try:\n",
    "                    dbutils.fs.cp(local_volume_path, target_staging_path)\n",
    "                    logger.info(f\"Copied to staging with dbutils: {local_volume_path} -> {target_staging_path}\")\n",
    "                    return\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"dbutils.fs.cp failed for {local_volume_path} -> {target_staging_path}: {e}\")\n",
    "\n",
    "                # Fallback: try local filesystem mapping (for /dbfs/ or dbfs:/ mapping)\n",
    "                try:\n",
    "                    if staging_dir.startswith('/dbfs/'):\n",
    "                        staging_local_dir = staging_dir\n",
    "                    elif staging_dir.startswith('dbfs:/'):\n",
    "                        staging_local_dir = '/dbfs' + staging_dir[len('dbfs:'):]\n",
    "                    else:\n",
    "                        staging_local_dir = staging_dir\n",
    "\n",
    "                    staging_local_path = os.path.join(staging_local_dir, base_name)\n",
    "                    os.makedirs(os.path.dirname(staging_local_path), exist_ok=True)\n",
    "                    shutil.copy2(local_dir, staging_local_path)\n",
    "                    logger.info(f\"Copied to staging by shutil: {local_dir} -> {staging_local_path}\")\n",
    "                    return\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Fallback copy failed: {local_dir} -> {staging_dir}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error in copy_to_staging for {local_dir} -> {staging_dir}: {e}\")\n",
    "\n",
    "    #NEW_END\n",
    "\n",
    "\n",
    "    ALL_TAGS = {}\n",
    "    resultados_MX = {}\n",
    "\n",
    "    adls_input_dir =  f'{adls_input_base_dir}/{date}'\n",
    "    adls_output_dir =  f'{adls_output_base_dir}/{date}'\n",
    "    local_input_dir = f'{local_dir}/swift_parsing/{date}'\n",
    "    volume_input_dir = f'file:{local_input_dir}'\n",
    "\n",
    "    local_output_dir = f'{local_dir}/swift_parsing_output/{date}'\n",
    "\n",
    "    # copiamos los ficheros a directorio volumen\n",
    "    try:\n",
    "        dbutils.fs.rm(volume_input_dir, True)\n",
    "        dbutils.fs.mkdirs(volume_input_dir)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error al borrar directorio {volume_input_dir} {e}')\n",
    "    dbutils.fs.cp(adls_input_dir, volume_input_dir, recurse=True)\n",
    "\n",
    "\n",
    "    ficheros_de_datos = os.listdir(Path(local_input_dir))\n",
    "\n",
    "    if len(ficheros_de_datos) == 0:\n",
    "        logger.info(f'No hay ficheros de datos en la carpeta: {volume_input_dir}')\n",
    "        return\n",
    "    for fichero in ficheros_de_datos:\n",
    "        csv_procesado = False\n",
    "        # Lee el primer archivo CSV encontrado\n",
    "        if fichero.endswith(\".csv\") and not csv_procesado:\n",
    "            fechas = pd.read_csv(f'{local_input_dir}/{fichero}')\n",
    "            # Muestra las primeras filas del DataFrame\n",
    "            fechas = fechas.rename(columns={'Warehouse ID': 'Filename'})\n",
    "            fechas = fechas.rename(columns={'Creation date': 'Creation_date'})\n",
    "        # Procesar el archivo .unknown\n",
    "        if fichero.endswith(\".unknown\"): separar_secciones(f'{local_input_dir}/{fichero}')\n",
    "\n",
    "    for filename in os.listdir(Path(local_input_dir)):\n",
    "        # Ignora archivos .unknown, .csv u ocultos (.crc que se generan automáticamente en dbfs, por ejemplo)\n",
    "        if filename[0] == '.' or filename.endswith(\".unknown\") or filename.endswith(\".csv\"): continue\n",
    "        # Procesa el archivo .fin\n",
    "        tags, resultados = ProcessMessage(local_input_dir, filename, date)\n",
    "\n",
    "        # Actualiza los diccionarios con los resultados\n",
    "        ALL_TAGS.update(tags)\n",
    "        resultados_MX.update(resultados)\n",
    "\n",
    "    os.makedirs(local_output_dir, exist_ok=True)\n",
    "    ruta_archivo_parquet_MT = local_output_dir + '/Payments' + '_' + datetime.now().strftime('%Y%m%d%H%M%S') + '.parquet'\n",
    "    ruta_archivo_parquet_MX = local_output_dir + '/Trade' + '_' + datetime.now().strftime('%Y%m%d%H%M%S') + '.parquet'\n",
    "    ruta_archivo_parquet_completo = local_input_dir + '/All_messages' + '_' + datetime.now().strftime('%Y%m%d%H%M%S') + '.parquet'\n",
    "\n",
    "    if ALL_TAGS:\n",
    "        data_list_MT = []\n",
    "        for key, value in ALL_TAGS.items():\n",
    "            row = {\"Filename\": key}\n",
    "            for subkey, subvalue in value.items():\n",
    "                if isinstance(subvalue, dict):\n",
    "                    for subsubkey, subsubvalue in subvalue.items():\n",
    "                        new_key = f\"{subkey}_{subsubkey}\"\n",
    "                        row[new_key] = subsubvalue\n",
    "                else:\n",
    "                    row[subkey] = subvalue\n",
    "            data_list_MT.append(row)\n",
    "        df_ALL_TAGS = pd.DataFrame(data_list_MT)\n",
    "        df_ALL_TAGS = separar_mensajes_101(df_ALL_TAGS)\n",
    "        df_ALL_TAGS = funciones_complejas(df_ALL_TAGS)\n",
    "        df_ALL_TAGS = df_ALL_TAGS.replace('\\n', ' ', regex=True)\n",
    "        df_ALL_TAGS['Filename'] = df_ALL_TAGS['Filename'].str.replace('.fin', '')\n",
    "        fechas_filtradas = fechas[['Filename', 'Creation_date']]\n",
    "        df_merged = pd.merge(df_ALL_TAGS, fechas_filtradas, on='Filename')\n",
    "        df_ALL_TAGS_final = cambiar_nombres_direccion(df_merged)\n",
    "        # df_ALL_TAGS_final.to_parquet(ruta_archivo_parquet_MT,index=False)\n",
    "        # df_ALL_TAGS_final.to_csv('nombre_del_archivo.csv',index=False, sep=';')\n",
    "\n",
    "    if resultados_MX:\n",
    "        data_list_MX = []\n",
    "        for key, value in resultados_MX.items():\n",
    "            row = {\"Filename\": key}\n",
    "            for subkey, subvalue in value.items():\n",
    "                if isinstance(subvalue, dict):\n",
    "                    for subsubkey, subsubvalue in subvalue.items():\n",
    "                        new_key = f\"{subkey}_{subsubkey}\"\n",
    "                        row[new_key] = subsubvalue\n",
    "                else:\n",
    "                    row[subkey] = subvalue\n",
    "            data_list_MX.append(row)\n",
    "        df_resultados_MX = pd.DataFrame(data_list_MX)\n",
    "        df_resultados_MX = New_MX_Names.rellenar_nan_con_prefijo(df_resultados_MX)\n",
    "        df_resultados_MX = df_resultados_MX.replace('\\n', ' ', regex=True)\n",
    "        df_resultados_MX['Filename'] = df_resultados_MX['Filename'].str.replace('.xml', '')\n",
    "        df_merged = pd.merge(df_resultados_MX, fechas, on='Filename', how='left')\n",
    "        df_resultados_MX_final = New_MX_Names.cambiar_nombres_MX(df_merged)\n",
    "        # df_resultados_MX_final.to_parquet(ruta_archivo_parquet_MX,index=False)\n",
    "        # df_resultados_MX_final.to_csv('prueba.csv',index=False,sep=';')\n",
    "\n",
    "    if 'df_resultados_MX_final' in locals() and 'df_ALL_TAGS_final' in locals():\n",
    "        columnas_comunes = df_resultados_MX_final.columns.intersection(df_ALL_TAGS_final.columns).tolist()\n",
    "        df_juntos = pd.merge(df_resultados_MX_final, df_ALL_TAGS_final, on=columnas_comunes, how='outer')\n",
    "        representaciones_nulas_1 = ['nan', 'NaN', 'None', ':None', '<NA>', '']\n",
    "        df_juntos = df_juntos.astype(str)\n",
    "        df_juntos.replace(representaciones_nulas_1, np.nan, inplace=True)\n",
    "        df_juntos = df_juntos.replace({'': np.nan, ' ': np.nan})\n",
    "        df_juntos.to_parquet(ruta_archivo_parquet_completo, index=False)\n",
    "        # payments = df_juntos[df_juntos['Message_type'].str.startswith(('pacs','camt','1', '2'))]\n",
    "        payments = df_juntos[\n",
    "            df_juntos['Message_type'].isin(lista_payments_message_type) | df_juntos['Message_type'].str.startswith(\n",
    "                ('pacs', 'camt'))]\n",
    "        if not payments.empty:\n",
    "            # payments = payments.dropna(axis=1, how='all')\n",
    "            payments_final = payments.reindex(columns=lista_payments)\n",
    "            payments_final.to_parquet(ruta_archivo_parquet_MT, index=False)\n",
    "            payments_final.to_csv('payments_final.csv', index=False, sep=';')\n",
    "            copy_to_staging(ruta_archivo_parquet_MT,adls_output_dir)\n",
    "\n",
    "        # trade = df_juntos[df_juntos['Message_type'].str.startswith(('7', 'tsrv'))]\n",
    "        trade = df_juntos[\n",
    "            df_juntos['Message_type'].isin(lista_trade_message_type) | df_juntos['Message_type'].str.startswith('tsrv')]\n",
    "        if not trade.empty:\n",
    "            # trade = trade.dropna(axis=1, how='all')\n",
    "            trade_final = trade.reindex(columns=lista_trade)\n",
    "            trade_final.to_parquet(ruta_archivo_parquet_MX, index=False)\n",
    "            trade_final.to_csv('trade_final.csv', index=False, sep=';')\n",
    "            copy_to_staging(ruta_archivo_parquet_MX,adls_output_dir)\n",
    "\n",
    "\n",
    "    if 'df_ALL_TAGS_final' in locals() and isinstance(locals()['df_ALL_TAGS_final'],\n",
    "                                                      pd.DataFrame) and 'df_resultados_MX_final' not in locals():\n",
    "        df_ALL_TAGS_final.to_parquet(ruta_archivo_parquet_completo, index=False)\n",
    "        representaciones_nulas = ['nan', 'NaN', 'None', ':None', '<NA>', '']\n",
    "        df_ALL_TAGS_final = df_ALL_TAGS_final.astype(str)\n",
    "        df_ALL_TAGS_final.replace(representaciones_nulas, np.nan, inplace=True)\n",
    "        # payments = df_ALL_TAGS_final[df_ALL_TAGS_final['Message_type'].str.startswith(('1', '2'))]\n",
    "        payments = df_ALL_TAGS_final[\n",
    "            df_ALL_TAGS_final['Message_type'].isin(lista_payments_message_type) | df_ALL_TAGS_final[\n",
    "                'Message_type'].str.startswith(('pacs', 'camt'))]\n",
    "        if not payments.empty:\n",
    "            # payments = payments.dropna(axis=1, how='all')\n",
    "            payments_final = payments.reindex(columns=lista_payments)\n",
    "            payments_final.to_parquet(ruta_archivo_parquet_MT, index=False)\n",
    "            copy_to_staging(ruta_archivo_parquet_MT,adls_output_dir)\n",
    "        # trade = df_ALL_TAGS_final[df_ALL_TAGS_final['Message_type'].str.startswith(('7'))]\n",
    "        trade = df_ALL_TAGS_final[df_ALL_TAGS_final['Message_type'].isin(lista_trade_message_type) | df_ALL_TAGS_final[\n",
    "            'Message_type'].str.startswith('tsrv')]\n",
    "        if not trade.empty:\n",
    "            # trade = trade.dropna(axis=1, how='all')\n",
    "            trade_final = trade.reindex(columns=lista_trade)\n",
    "            trade_final.to_parquet(ruta_archivo_parquet_MX, index=False)\n",
    "            copy_to_staging(ruta_archivo_parquet_MX,adls_output_dir)\n",
    "\n",
    "    if 'df_resultados_MX_final' in locals() and isinstance(locals()['df_resultados_MX_final'],\n",
    "                                                           pd.DataFrame) and 'df_ALL_TAGS_final' not in locals():\n",
    "        df_resultados_MX_final.to_parquet(ruta_archivo_parquet_completo, index=False)\n",
    "        representaciones_nulas = ['nan', 'NaN', 'None', ':None', '<NA>', '']\n",
    "        df_resultados_MX_final = df_resultados_MX_final.astype(str)\n",
    "        df_resultados_MX_final.replace(representaciones_nulas, np.nan, inplace=True)\n",
    "        payments = df_resultados_MX_final[df_resultados_MX_final['Message_type'].str.startswith(('pacs', 'camt'))]\n",
    "        if not payments.empty:\n",
    "            # payments = payments.dropna(axis=1, how='all')\n",
    "            payments_final = payments.reindex(columns=lista_payments)\n",
    "            payments_final.to_parquet(ruta_archivo_parquet_MT, index=False)\n",
    "            copy_to_staging(ruta_archivo_parquet_MT,adls_output_dir)\n",
    "        trade = df_resultados_MX_final[df_resultados_MX_final['Message_type'].str.startswith(('tsrv'))]\n",
    "        if not trade.empty:\n",
    "            # trade = trade.dropna(axis=1, how='all')\n",
    "            trade_final = trade.reindex(columns=lista_trade)\n",
    "            trade_final.to_parquet(ruta_archivo_parquet_MX, index=False)\n",
    "            copy_to_staging(ruta_archivo_parquet_MX,adls_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20134545-06bf-4070-b772-12ab6ee4de09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "local_dir = \"/tmp/cumplimiento/\"\n",
    "\n",
    "adls_input_base_dir = dbutils.widgets.get(\"adls_input_base_dir\")\n",
    "adls_output_base_dir = dbutils.widgets.get(\"adls_output_base_dir\")\n",
    "date = dbutils.widgets.get(\"date\")\n",
    "\n",
    "#Comprobamos que hay ficheros en la ruta\n",
    "assert(len(dbutils.fs.ls(adls_input_base_dir + \"/\" + date)) != 0)\n",
    "dbutils.fs.ls(adls_input_base_dir + \"/\" + date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c534ddca-2a5d-4302-9d83-a25b7c48031e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutamos main finalmente\n",
    "main(adls_input_base_dir, adls_output_base_dir, date, local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e64c2bb7-af0c-4888-aca7-60d62af80f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(adls_output_base_dir + \"/\" + date)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SWIFT_parsing_refactor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}